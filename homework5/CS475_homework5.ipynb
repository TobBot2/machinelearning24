{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tj6i9Vtn_7Z5"
   },
   "source": [
    "# **Homework 5 Practicum**\n",
    "# Version 1.0 (Nov 15, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5AyH9zp_7Z7"
   },
   "source": [
    "<font color='blue'> TODO:</font> Trevor Black (tblack20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4T8-gmjN_7Z7"
   },
   "source": [
    "Instructions:\n",
    "This notebook has two parts:\n",
    "\n",
    "Part 1: Implement Principal Component Analysis (PCA) and K-Nearest Neghbors (KNN)\n",
    "\n",
    "Part 2: Implement Q Learning algorithm.\n",
    "\n",
    "Please note that in this practicum, we only require code implementation without any usual questions.\n",
    "\n",
    "Please <font color='blue'>make a copy of this notebook in your own drive</font> before you make any edits. You can do so through File -> Save a copy in Drive\n",
    "\n",
    "NOTE: Submit notebook on gradescope. You can run autograder as many times as needed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "OhdQxVTs_7Z8"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs,make_classification,load_wine,load_iris,load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.decomposition import PCA as SKLearnPCA\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "5fFKvcYL_7Z-"
   },
   "source": [
    "\n",
    "# **PART I: Implement Principal Component Analysis (PCA) and K-Nearest Neghbors (KNN)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BWNevkH9_7Z-"
   },
   "source": [
    "Principal Component Analysis (PCA) is a widely used technique in machine learning for dimensionality reduction. It aims to reduce the number of variables (features) in a dataset while preserving as much of the original information as possible.\n",
    "\n",
    "PCA identifies the \"directions\" (called principal components) along which the data varies the most. These components are ordered by the amount of variance they explain in the dataset.This helps in visualizing data, improving model accuracy, and reducing overfitting.\n",
    "In this implementation, we will create our own custom PCA class, compare its performance with the PCA implementation from scikit-learn, and demonstrate its use in a classification task. This will help us understand the trade-offs between using PCA and working with the original, high-dimensional data.\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a simple and intuitive supervised machine learning algorithm used for both classification and regression tasks. It works by classifying a data point based on the majority class (for classification) of its K closest neighbors in the feature space, where distance between points is typically measured using Euclidean distance.\n",
    "KNN is a lazy learner, meaning it doesn't build an explicit model during training, but instead uses the entire training dataset to make predictions at runtime. While KNN is easy to implement and doesn't require a lot of training, it can be computationally expensive, especially with large datasets, and is sensitive to the choice of K and irrelevant features. Feature scaling is important to improve its performance, and the algorithm may struggle with high-dimensional data due to the \"curse of dimensionality.\" Hence, we will compare the performance of KNN with and without PCA.\n",
    "\n",
    "Things todo:\n",
    "\n",
    "1. Implement fit_transform and transform methods of MyPCA class.\n",
    "2. Implement predict method of KNN class\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6alihQlY9Aqu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DONOT MODIFY\n",
    "def generate_synthetic_data(num_samples=1000, num_features=100, num_classes=2):\n",
    "    \"\"\"\n",
    "    Generate synthetic data using make_classification function.\n",
    "\n",
    "    Parameters:\n",
    "    - n_samples (int): Number of samples to generate.\n",
    "    - n_features (int): Number of features for each sample.\n",
    "    - centers (int): Number of centers for blobs.\n",
    "    - random_state (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "    - X (ndarray): The generated synthetic data.\n",
    "    - y (ndarray): The labels (not needed for PCA, but useful for classification tasks).\n",
    "    \"\"\"\n",
    "\n",
    "    X,y = make_classification(n_samples=num_samples, n_features=num_features, n_informative=20, n_classes=num_classes, random_state=42)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NVd7_J7N9dVw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyPCA:\n",
    "    \"\"\"\n",
    "    A custom PCA implementation for dimensionality reduction.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_components=None):\n",
    "        \"\"\"\n",
    "        Initialize the PCA object.\n",
    "\n",
    "        Parameters:\n",
    "        - n_components (int or None): Number of components to keep. If None, all components are kept.\n",
    "        \"\"\"\n",
    "        self.n_components = n_components\n",
    "        self.mu = None  # Mean of the data\n",
    "        self.V_m = None  # Matrix of principal components\n",
    "        self.sorted_eigenvalues = None  # Sorted eigenvalues\n",
    "\n",
    "    def fit_transform(self, D):\n",
    "        \"\"\"\n",
    "        Perform PCA on the dataset and return the transformed data.\n",
    "\n",
    "        Parameters:\n",
    "        - D (ndarray): The input data (n_samples x n_features).\n",
    "\n",
    "        Returns:\n",
    "        - D_tilde (ndarray): The transformed dataset (n_samples x n_components).\n",
    "        \"\"\"\n",
    "        \n",
    "        # steps outlined on slide 15.5\n",
    "\n",
    "        # step 1, estimate mean vectors\n",
    "        self.mu = np.mean(D, axis=0) # take mean on rows\n",
    "\n",
    "        # step 2, zero-center data\n",
    "        zeroed = D - self.mu\n",
    "\n",
    "        # step 3, calculate empirical covariance matrix\n",
    "        Cov = 1/(D.size - 1) * zeroed.transpose() @ zeroed\n",
    "\n",
    "        # step 4, compute eigendecomposition of covariance matrix\n",
    "        eigen_vals, eigen_vecs = np.linalg.eigh(Cov)\n",
    "\n",
    "        # step 5, sort eigen matrices\n",
    "        sorted_indices = np.argsort(eigen_vals)\n",
    "        self.sorted_eigenvalues = eigen_vals[sorted_indices]\n",
    "        V = eigen_vecs[sorted_indices]\n",
    "\n",
    "        # step 6, pick m < k largest eigenvalues\n",
    "        self.V_m = V[:, :self.n_components]\n",
    "\n",
    "        # step 7, project centered data\n",
    "        D_tilde = zeroed @ self.V_m\n",
    "\n",
    "        return D_tilde\n",
    "\n",
    "    def transform(self, D):\n",
    "        \"\"\"\n",
    "        Transform the data using the fitted PCA model.\n",
    "\n",
    "        Parameters:\n",
    "        - D (ndarray): The input data (n_samples x n_features).\n",
    "\n",
    "        Returns:\n",
    "        - D_tilde (ndarray): The transformed dataset (n_samples x n_components).\n",
    "        \"\"\"\n",
    "        zeroed = D - self.mu\n",
    "        D_tilde = zeroed @ self.V_m\n",
    "        return D_tilde\n",
    "\n",
    "    def explained_variance(self):\n",
    "        \"\"\"\n",
    "        Calculate the explained variance ratio for the selected principal components.\n",
    "\n",
    "        The total variance is the sum of the eigenvalues of all the principal components.\n",
    "        This represents the overall spread of the data in the original feature space.\n",
    "\n",
    "        The explained variance is the sum of the eigenvalues corresponding to the top n_components principal components.\n",
    "        This represents the spread of the data along the selected components.\n",
    "\n",
    "        Hence, the ratio gives us an idea of how much of the original data's variance is retained by the selected components.\n",
    "        Returns:\n",
    "        - explained_variance (float): The proportion of variance explained by the selected components.\n",
    "        \"\"\"\n",
    "        total_variance = np.sum(self.sorted_eigenvalues)\n",
    "        explained_variance = np.sum(self.sorted_eigenvalues[:self.n_components])\n",
    "        return explained_variance / total_variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "TXeO1bxX9hlw",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyPCA Explained Variance: 0.0935\n",
      "sklearn PCA Explained Variance: 0.3301\n"
     ]
    }
   ],
   "source": [
    "#DONOT MODIFY\n",
    "\n",
    "def compare_pca(num_components):\n",
    "    \"\"\"\n",
    "    Compare the explained variance of MyPCA and sklearn PCA on synthetic dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - X (ndarray): The input data (n_samples x n_features).\n",
    "    \"\"\"\n",
    "    X = generate_synthetic_data(num_samples=1000, num_features=100, num_classes=2)[0]\n",
    "    # Standardize the dataset\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Apply your PCA (MyPCA)\n",
    "    my_pca = MyPCA(n_components=num_components)\n",
    "    X_pca = my_pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Print MyPCA explained variance\n",
    "    print(f\"MyPCA Explained Variance: {my_pca.explained_variance():.4f}\")\n",
    "\n",
    "    # Apply sklearn PCA\n",
    "    sklearn_pca = SKLearnPCA(n_components=num_components)\n",
    "    X_pca_sklearn = sklearn_pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Print sklearn PCA explained variance\n",
    "    print(f\"sklearn PCA Explained Variance: {np.sum(sklearn_pca.explained_variance_ratio_):.4f}\")\n",
    "    return my_pca.explained_variance(), np.sum(sklearn_pca.explained_variance_ratio_)\n",
    "\n",
    "_,_ = compare_pca(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbRkwSCl9n_n",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class KNN:\n",
    "    def __init__(self, k=5):\n",
    "        \"\"\"\n",
    "        KNN classifier initialization.\n",
    "\n",
    "        :param k: Number of neighbors to consider when making a prediction\n",
    "        \"\"\"\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Store training data.\n",
    "\n",
    "        :param X_train: Training data features (2D numpy array)\n",
    "        :param y_train: Training data labels (1D numpy array)\n",
    "        \"\"\"\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        \"\"\"\n",
    "        Predict the label for each test point in X_test.\n",
    "\n",
    "        :param X_test: Test data features (2D numpy array)\n",
    "        :return: Predicted labels (1D numpy array)\n",
    "        \"\"\"\n",
    "        predictions = [self._predict(x) for x in X_test]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the label for a single test point.\n",
    "\n",
    "        :param x: Single test data point (1D numpy array)\n",
    "        :return: Predicted label (scalar)\n",
    "        \"\"\"\n",
    "        # get distances of x to each X in training data\n",
    "        distances = [self._euclidean_distance(x, x_train) for x_train in self.X_train]\n",
    "\n",
    "        # get k nearest neighbors\n",
    "        k_nearest_indices = np.argsort(distances)[:self.k]\n",
    "\n",
    "        # get corresponding labels\n",
    "        labels = self.y_train[k_nearest_indices]\n",
    "\n",
    "        # get most common label\n",
    "        c = Counter(labels)\n",
    "        return c.most_common(1)[0][0] # first most common, first element (gets the value, excludes # occurences)\n",
    "\n",
    "    def _euclidean_distance(self, x1, x2):\n",
    "        \"\"\"\n",
    "        Compute the Euclidean distance between two points.\n",
    "\n",
    "        :param x1: First point (1D numpy array)\n",
    "        :param x2: Second point (1D numpy array)\n",
    "        :return: Euclidean distance (float)\n",
    "        \"\"\"\n",
    "        return np.sqrt(np.sum((x1 - x2) ** 2))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "t2t0sWmQ9sJ4",
    "otter": {
     "tests": [
      "PCA"
     ]
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape =(5000, 200)\n",
      "Accuracy before PCA: 0.6940\n",
      "Accuracy after PCA(5 components): 0.5893\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#DONOT MODIFY; Note: On colab it took 4-5min to run\n",
    "def classification_with_pca():\n",
    "    \"\"\"\n",
    "    Use PCA for dimensionality reduction in classification task using KNN.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate the synthetic data\n",
    "    X , y = generate_synthetic_data(num_samples=5000, num_features=200, num_classes=2)\n",
    "    # Step 2: Split the data into training and testing sets\n",
    "    print(f\"Data shape ={X.shape}\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    # Step 3: Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    # Step 4: Apply KNN without PCA and evaluate the performance\n",
    "    knn = KNN()\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy_before_pca = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Accuracy before PCA: {accuracy_before_pca:.4f}\")\n",
    "    # Step 5: Apply KNN with PCA and evaluate the performance\n",
    "    pca = MyPCA(n_components=20)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    knn_pca = KNN()\n",
    "    knn_pca.fit(X_train_pca, y_train)\n",
    "    y_pred_pca = knn_pca.predict(X_test_pca)\n",
    "    accuracy_after_pca = accuracy_score(y_test, y_pred_pca)\n",
    "    print(f\"Accuracy after PCA(5 components): {accuracy_after_pca:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "classification_with_pca()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6P245tqP_7aE"
   },
   "source": [
    "# **PART II: Reinforcement learning**\n",
    "\n",
    "Q-learning is a model-free reinforcement learning algorithm used to find the optimal action-selection policy for any given Markov decision process (MDP). It works by learning an action-value function that gives the expected utility of taking a particular action in a particular state and following the optimal policy thereafter\n",
    "\n",
    "We will solve a control problem in this excercise (Balance Cartpole). The goal is to balance a pole on a cart by moving the cart left or right. The state space consists of the cart's position, velocity, pole angle, and pole angular velocity. The agent can take two actions: move the cart left or right. The agent receives a reward of +1 for each time step the pole is balanced, and the episode ends when the pole falls beyond a certain angle or the cart moves out of bounds.\n",
    "\n",
    "More information about the cartpole environment can be found here: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "\n",
    "\n",
    "Since the state in this problem is continuous, we will discretize the state space using a grid; see `get_discrete_state` function.\n",
    "\n",
    "Things to do in this part:\n",
    "1. Understand the interaction flow of the gym environment. A simple usage can be found in the main page of the gym library: https://gymnasium.farama.org/\n",
    "\n",
    "2. Your main task is to implement the `train_qlearning` function. You have the flexibility to implement the Q-learning algorithm as you see fit, as long as it functions correctly. The evaluation will be based on the total reward obtained using your qTable. Refer to `evaluation_qlearning` for details.\n",
    "\n",
    "We also provide the `save_to_gif` function to view how your q learning perform in the simulation\n",
    "\n",
    "\n",
    "The gradescope test case will be based on `evaluation_qlearning`. You will pass if the **average total reward** exceeds the required threshold.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6WKQI0B38YHi"
   },
   "source": [
    "Use the following command to install the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "PWL-xylX-2P2"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "vgNj-7Lm-sLW"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "APz2-szpig-4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.00448892 -0.03126102  0.02695922  0.04819566]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "state, _ = env.reset()\n",
    "print(state) # state is a real vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "dL2IdQt2-sLW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do not change this cell\n",
    "STATE_SIZE = 4\n",
    "NUM_BIN = 20\n",
    "# Get the size of each bucket\n",
    "bins = [\n",
    "\tnp.linspace(-4.8, 4.8, NUM_BIN),\n",
    "\tnp.linspace(-4, 4, NUM_BIN),\n",
    "\tnp.linspace(-.418, .418, NUM_BIN),\n",
    "\tnp.linspace(-4, 4, NUM_BIN)\n",
    "]\n",
    "# Given a state of the environment, return its discreteState index in qTable\n",
    "def get_discrete_state(state):\n",
    "    stateIndex = []\n",
    "    for i in range(STATE_SIZE):\n",
    "        stateIndex.append(np.digitize(state[i], bins[i]) - 1)  # -1 will turn bin into index\n",
    "\n",
    "    bin_number = 0\n",
    "    for i, v in enumerate(stateIndex):\n",
    "        bin_number += v * NUM_BIN**i\n",
    "    return bin_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "HElcrZRU-sLW",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This is just a suggestion template, you're free to implement\n",
    "# this function however you want, do not change the function signature,\n",
    "# it will failed the autograder.\n",
    "def train_qlearning(env, episodes=1000):\n",
    "\tqTable = np.random.uniform(low=-2, high=0, size=([NUM_BIN ** STATE_SIZE ] + [2])) # random initialize table\n",
    "\t# Define some constants that you want to use\n",
    "\tLEARNING_RATE = 0.1\n",
    "\tDISCOUNT = 0.95\n",
    "\treward_per_episode = [] # for keep track of the reward\n",
    "\n",
    "\tfor i_episode in range(episodes):\n",
    "\t\tstate, _ = env.reset()\n",
    "\t\tstate = get_discrete_state(state) # convert continuous state to discrete state\n",
    "\t\tdone = False  # has the enviroment finished?\n",
    "\t\treward_episode = 0  # how may movements cart has made\n",
    "\n",
    "\t\twhile not done:\n",
    "\t\t\t# your epsilon greedy policy\n",
    "\t\t\t...\n",
    "\t\t\tnext_state, reward, done, _, _ = env.step(action)  # perform action\n",
    "\t\t\treward_episode += reward # keep track of the total reward of this episode\n",
    "\t\t\t# Update Q table\n",
    "\t\t\t...\n",
    "\t\t\tif done:\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\t\t\t...\n",
    "\t\treward_per_episode.append(reward_episode)\n",
    "\n",
    "\t\t# Add new metrics for graph\n",
    "\t\tif i_episode % 100 == 0:\n",
    "\t\t\tlatest_runs = reward_per_episode[-100:]\n",
    "\t\t\taverage_reward = sum(latest_runs) / len(latest_runs)\n",
    "\t\t\tprint(f\"Episode {i_episode}, Average Reward: {average_reward}\")\n",
    "\n",
    "\tenv.close()\n",
    "\n",
    "\treturn qTable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "VrpVTH94DbtY",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We use this function to test on gradescope, don't change it\n",
    "def evaluate_qlearning(qTable, env, episodes=100):\n",
    "    scores = []\n",
    "    for _ in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        discreteState = get_discrete_state(state)\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action = np.argmax(qTable[discreteState])\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            discreteState = get_discrete_state(state)\n",
    "            score += reward\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "HCNoi7uZpfbW",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# #Uncomment to test your qlearning implementation\u001b[39;00m\n\u001b[0;32m      2\u001b[0m env \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCartPole-v1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m qTable \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_qlearning\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(evaluate_qlearning(qTable, env)) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m30\u001b[39m \u001b[38;5;66;03m# you should see the average reward increasing.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[26], line 20\u001b[0m, in \u001b[0;36mtrain_qlearning\u001b[1;34m(env, episodes)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     18\u001b[0m \t\u001b[38;5;66;03m# your epsilon greedy policy\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \t\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m---> 20\u001b[0m \tnext_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(\u001b[43maction\u001b[49m)  \u001b[38;5;66;03m# perform action\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \treward_episode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward \u001b[38;5;66;03m# keep track of the total reward of this episode\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \t\u001b[38;5;66;03m# Update Q table\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'action' is not defined"
     ]
    }
   ],
   "source": [
    "# #Uncomment to test your qlearning implementation\n",
    "env = gym.make('CartPole-v1')\n",
    "qTable = train_qlearning(env, episodes=3000)\n",
    "assert np.mean(evaluate_qlearning(qTable, env)) > 30 # you should see the average reward increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "wVmquZ8N-sLX",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqdm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimageio\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_to_gif\u001b[39m(filename, qTable):\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#Make gym env\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tqdm'"
     ]
    }
   ],
   "source": [
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "from tqdm import trange\n",
    "from IPython.display import Image\n",
    "\n",
    "def save_to_gif(filename, qTable):\n",
    "    #Make gym env\n",
    "    env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "    #Run the env\n",
    "    state, _ = env.reset()\n",
    "    frames = []\n",
    "    for t in trange(200):\n",
    "        #Render to frames buffer\n",
    "        frames.append(env.render())\n",
    "        action = qTable[get_discrete_state(state)].argmax()\n",
    "        state, _, done, _, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    env.close()\n",
    "    imageio.mimsave(filename, frames, duration=0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kI8K6Xir-sLY",
    "otter": {
     "tests": [
      "rl"
     ]
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #Uncomment to view the gif\n",
    "# save_to_gif('record.gif', qTable)\n",
    "# Image(open('record.gif','rb').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3UyU_vfChZsd"
   },
   "source": [
    "## Feedback\n",
    "\n",
    "Please provide us with some feedback on how long each section or this homework overall took you. Any other feedback is also welcomed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVbMEfNuhZsd"
   },
   "source": [
    "## Submit\n",
    "Great work! You're all done.\n",
    "\n",
    "Make sure to submit this Python notebook. See the homework writeup for directions."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  },
  "otter": {
   "OK_FORMAT": false,
   "assignment_name": "hw05",
   "tests": {
    "PCA": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"PCA\"\npoints = None\n\n@test_case(points=4, hidden=False, \n    success_message=\"Test for PCA with 10 components.\", \n    failure_message=\"Test for PCA with 10 components.\")\ndef test_compare_pca1(np, compare_pca):\n    \"\"\"\n    Test the compare_pca function.\n    \"\"\"\n    explained_variance_mypca, explained_variance_sklearn = compare_pca(num_components=10)\n    assert abs(explained_variance_mypca - explained_variance_sklearn) < 0.01, 'Explained variance difference is too large'\n\n@test_case(points=4, hidden=False, \n    success_message=\"Test for PCA with 20 components.\", \n    failure_message=\"Test for PCA with 20 components.\")\ndef test_compare_pca2(np, compare_pca):\n    \"\"\"\n    Test the compare_pca function.\n    \"\"\"\n    explained_variance_mypca, explained_variance_sklearn = compare_pca(num_components=20)\n    assert abs(explained_variance_mypca - explained_variance_sklearn) < 0.01, 'Explained variance difference is too large'\n\n@test_case(points=4, hidden=False, \n    success_message=\"Test for KNN with Iris dataset.\", \n    failure_message=\"Test for KNN with Iris dataset.\")\ndef test_knn_predict_iris(np, load_iris, train_test_split, KNN, accuracy_score):\n    iris = load_iris()\n    X, y = (iris.data, iris.target)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    knn = KNN(k=5)\n    knn.fit(X_train, y_train)\n    predictions = knn.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f'Iris dataset accuracy: {accuracy}')\n    assert accuracy > 0.9, f'Accuracy on Iris dataset is too low. Expected >0.9, got {accuracy}'\n\n@test_case(points=4, hidden=False, \n    success_message=\"Test for KNN with Wine dataset.\", \n    failure_message=\"Test for KNN with Wine dataset.\")\ndef test_knn_predict_wine(np, load_wine, train_test_split, KNN, accuracy_score):\n    wine = load_wine()\n    X, y = (wine.data, wine.target)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    knn = KNN(k=5)\n    knn.fit(X_train, y_train)\n    predictions = knn.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f'Wine dataset accuracy: {accuracy}')\n    assert accuracy >= 0.75, f'Accuracy on Wine dataset is too low. Expected >0.75, got {accuracy}'\n\n@test_case(points=4, hidden=False, \n    success_message=\"Test for KNN with Digits dataset.\", \n    failure_message=\"Test for KNN with Digits dataset.\")\ndef test_knn_predict_digits(np, load_digits, train_test_split, KNN, accuracy_score):\n    digits = load_digits()\n    X, y = (digits.data, digits.target)\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n    knn = KNN(k=5)\n    knn.fit(X_train, y_train)\n    predictions = knn.predict(X_test)\n    accuracy = accuracy_score(y_test, predictions)\n    print(f'Digits dataset accuracy: {accuracy}')\n    assert accuracy > 0.9, f'Accuracy on Digits dataset is too low. Expected >0.9, got {accuracy}'\n\n",
    "rl": "from otter.test_files import test_case\n\nOK_FORMAT = False\n\nname = \"rl\"\npoints = None\n\n@test_case(points=5, hidden=False, \n    success_message=\"K-mean (test case 1).\", \n    failure_message=\"K-mean (test case 1).\")\ndef test_rl1(train_qlearning, evaluate_qlearning, gym, np):\n    env = gym.make('CartPole-v1')\n    qTable = train_qlearning(env, 3000)\n    assert np.mean(evaluate_qlearning(qTable, env)) > 50\n\n"
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
