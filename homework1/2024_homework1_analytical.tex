\documentclass[11pt]{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage[fleqn]{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[many]{tcolorbox}
\usepackage{lipsum}
\usepackage{float}
\usepackage{trimclip}
\usepackage{listings}
\usepackage{environ}% http://ctan.org/pkg/environ
\usepackage{wasysym}
\usepackage{array}


\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm

\newcommand{\vwi}{{\bf w}_i}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}
\newcommand{\defeq}{\overset{\text{def}}{=}}
\renewcommand{\vec}[1]{\mathbf{#1}}



% \bgroup
\def\arraystretch{1.5}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{z}[1]{>{\centering\arraybackslash}m{#1}}

%Arguments are 1 - height, 2 - box title
\newtcolorbox{textanswerbox}[2]{%
 width=\textwidth,colback=white,colframe=blue!30!black,floatplacement=H,height=#1,title=#2,clip lower=true,before upper={\parindent0em}}

 \newtcolorbox{eqanswerbox}[1]{%
 width=#1,colback=white,colframe=black,floatplacement=H,height=3em,sharp corners=all,clip lower=true,before upper={\parindent0em}}

 %Arguments are 1 - height, 2 - box title
 \NewEnviron{answertext}[2]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
        \BODY
        \end{textanswerbox}
        }
        }
}

%Arguments are 1 - height, 2 - box title, 3 - column definition
 \NewEnviron{answertable}[3]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
                \vspace{-0.5cm}
                        \begin{table}[H]
                        \centering
                        \begin{tabular}{#3}
                                \BODY
                        \end{tabular}
                        \end{table}
        \end{textanswerbox}
        }
        }
}

 %Arguments are 1 - height, 2 - box title, 3 - title, 4- equation label, 5 - equation box width
 \NewEnviron{answerequation}[5]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
                \vspace{-0.5cm}
                        \begin{table}[H]
                        \centering
                \renewcommand{\arraystretch}{0.5}% Tighter

                        \begin{tabular}{#3}
                                #4 =	&
                        \clipbox{0pt 0pt 0pt 0pt}{

                        \begin{eqanswerbox}{#5}
                                $\BODY$
                        \end{eqanswerbox}
                        } \\
                        \end{tabular}
                        \end{table}

        \end{textanswerbox}
        }
        }
}

 %Arguments are 1 - height, 2 - box title
 \NewEnviron{answerderivation}[2]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
        \BODY
        \end{textanswerbox}
        }
        }
}

\newcommand{\Checked}{{\LARGE \XBox}}%
\newcommand{\Unchecked}{{\LARGE \Square}}%
\newcommand{\TextRequired}{{\textbf{Place Answer Here}}}%
\newcommand{\EquationRequired}{\textbf{Type Equation Here}}%


\newcommand{\answertextheight}{5cm}
\newcommand{\answertableheight}{4cm}
\newcommand{\answerequationheight}{2.5cm}
\newcommand{\answerderivationheight}{14cm}

\newcounter{QuestionCounter}
\newcounter{SubQuestionCounter}[QuestionCounter]
\setcounter{SubQuestionCounter}{1}

\newcommand{\subquestiontitle}{Question \theQuestionCounter.\theSubQuestionCounter~}
\newcommand{\newquestion}{\stepcounter{QuestionCounter}\setcounter{SubQuestionCounter}{1}\newpage}
\newcommand{\newsubquestion}{\stepcounter{SubQuestionCounter}}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\indices}{indices}
\DeclareMathOperator{\Bernoulli}{Bernoulli}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}

\lstset{language=[LaTeX]TeX,basicstyle=\ttfamily\bf}

\pagestyle{myheadings}
\markboth{Homework 1}{Fall 2024 CS 475/675 Machine Learning: Homework 1}

\title{CS 475 Machine Learning: Homework 1 Analytical \\
(50 points)\\
\Large{Assigned: Friday, September 6, 2024} \\
\Large{Due: Friday, September 20, 2024, 11:59 pm US/Eastern}}
\author{Trevor Black (tblack20)}
\date{}

\begin{document}
\maketitle
\thispagestyle{headings}

\section*{Instructions }
We have provided this \LaTeX{} document for turning in this homework. We give you one or more boxes to answer each question.  The question to answer for each box will be noted in the title of the box.  You can change the size of the box if you need more space.\\

{\bf Other than your name, do not type anything outside the boxes. Leave the rest of the document unchanged.}\\


\textbf{Do not change any formatting in this document, or we may be unable to
  grade your work. This includes, but is not limited to, the height of
  textboxes, font sizes, and the spacing of text and tables.  Additionally, do
  not add text outside of the answer boxes. Entering your answers are the only
  changes allowed.}\\


\textbf{We strongly recommend you review your answers in the generated PDF to
  ensure they appear correct. We will grade what appears in the answer boxes in
  the submitted PDF, NOT the original latex file.}

% \section*{ Notation}
% {
% \centering
% \smallskip\begin{tabular}{r l}
% \(\vec{x_i}\) & One input data vector. \(\vec{x_i}\) is \(M\) dimensional.
% \(\vec{x_i} \in \mathbb{R}^{1 \times M}\).  \\ &
% We assume $\vec{x_i}$ is augmented with a  $1$ to include a bias term. \\ \\
% \(\vec{X}\) & 	A matrix of concatenated \(\vec{x_i}\)'s. There are \(N\) input vectors, so \(\vec{X} \in \mathbb{R}^{N \times M}\) \\ \\
% \(y_i\) & The true label for input vector \(\vec{x_i}\). In regression problems, \(y_i\) is continuous. \\ & In general ,\(y_i\) can be a vector, but for now we assume it's a scalar: \(y_i \in \mathbb{R}^1\). \\ \\

% \(\vec{y}\) & 	A vector of concatenated \(y_i\)'s. There are \(N\) input vectors, so \(\vec{y} \in \mathbb{R}^{N \times 1}\) \\ \\

% \(\vec{w}\) & A weight vector. We are trying to learn the elements of \(\vec{w}\). \\
% & \(\vec{w}\) is the same number of elements as \(\vec{x_i}\) because we will end up computing \\
% & the dot product \(\vec{x_i} \cdot \vec{w}\). \\
% & \(\vec{w} \in \mathbb{R}^{M \times 1}\). We assume the bias term is included in \(\vec{w}\). \\ \\

% \(h(\vec(x))\) & The true regression function that describes the data. \\ \\
 
% i.i.d. & Independently and identically distributed. \\ \\

% Bias-variance  & We can write \(E_D[(f(x, D) - h(x))^2]\) = \\
% decomposition  & \((E_D[f(x, D) - h(x))^2 + E_D[(f(x, D) - E_D[f(x, D)])^2]\) \\
%                             & where the first term is the bias squared, and the second term is the variance.\\ \\

%  Notes: & In general, a lowercase letter (not boldface), $a$, indicates a scalar. \\
%   & A boldface lowercase letter, $\vec{a}$, indicates a vector. \\  &  A boldface uppercase letter, $\vec{A}$, indicates a matrix. \\
% \end{tabular}
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Maximum Likelihood [10 pts]} 


\begin{enumerate}
\item (5 pts) Given a dataset of 
$n$ independently and identically distributed data points $\{x_i\}_{i=1}^n$, where each $x_i$ is drawn from a Gaussian distribution, $x_i\sim \mathcal{N}(\mu, \sigma^2)$. Show that the MLE estimator of the mean is:
\begin{align}
& \hat{\mu}=\frac{1}{n} \sum_{i=1}^n x_i 
\end{align}
\begin{answertext}{9cm}{}

Likelihood\\
$\mathcal{L}(\mu, \sigma^2 | \{x_i\}_{i=1}^{n}) = \prod_{i=1}^{n} p(x_i | \mu, \sigma^2)$\\
$p(x_i | \mu, \sigma^2) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp({-\frac{(x_i-\mu)^2}{2\sigma^2}})$\\
$\mathcal{L}(\mu, \sigma^2 | \{x_i\}_{i=1}^{n}) = \prod_{i=1}^{n} \frac{1}{\sqrt{2\pi\sigma^2}} \exp({-\frac{(x_i-\mu)^2}{2\sigma^2}}) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n \exp(-\sum_{i=1}^{n}\frac{(x_i-\mu)^2}{2\sigma^2})$\\

Log likelihood\\
$\log{\mathcal{L}(\mu, \sigma^2 | \{x_i\}_{i=1}^{n})} = \ell(\mu, \sigma^2 | \{x_i\}_{i=1}^{n}) = \log((\frac{1}{\sqrt{(2\pi\sigma^2)}})^n \exp(-\sum_{i=1}^{n}\frac{(x_i-\mu)^2}{2\sigma^2}))$\\
$\ell(\mu, \sigma^2 | \{x_i\}_{i=1}^{n}) = n\log(\frac{1}{\sqrt{2\pi\sigma^2}}) - \frac{1}{2\sigma^2} \sum_{i=1}^{n}(x_i-\mu)^2 = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu)^2$\\

Maximize log likelihood for $\mu$\\
$\frac{d}{d\mu} \ell(\mu, \sigma^2 | \{x_i\}_{i=1}^{n}) = \frac{1}{\sigma^2}\sum_{i=1}^{n}(x_i-\mu) = 0$\\
$0 = \frac{1}{\sigma^2}\sum_{i=1}^{n}(x_i-\mu) = \sum_{i=1}^{n}(x_i-\mu) = -n\mu + \sum_{i=1}^{n}(x_i)$\\
$\mu = \frac{1}{n}\sum_{i=1}^{n}x_i$\\

So...
$\hat{\mu} = \frac{1}{n}\sum_{i=1}^{n}x_i$

\end{answertext} 
\newpage
\item (5 pts) Show that the MLE estimator of the variance is: 
\begin{align}
& \hat{\sigma}^2=\frac{1}{n} \sum_{i=1}^n\left(x_i-\widehat{\mu}\right)^2
\end{align}
\begin{answertext}{9cm}{}

Likelihood (see previous question for details)\\
$\mathcal{L}(\mu, \sigma^2 | \{x_i\}_{i=1}^{n}) = (\frac{1}{\sqrt{2\pi\sigma^2}})^n \exp(-\sum_{i=1}^{n}\frac{(x_i-\mu)^2}{2\sigma^2})$\\

Log likelihood (see previous question for details)\\
$\ell(\mu, \sigma^2 | \{x_i\}_{i=1}^{n}) = -\frac{n}{2}\log(2\pi\sigma^2) - \frac{1}{2\sigma^2}\sum_{i=1}^{n}(x_i-\mu)^2$\\

Maximize log likelihood for $\sigma^2$\\
$\frac{d}{d\sigma^2}\ell(\mu, \sigma^2 | \{x_i\}_{i=1}^{n}) = -\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}\sum_{i=1}^{n}(x_i-\mu)^2 = 0$\\
$0 = -n\sigma^2+\sum_{i=1}^{n}(x_i-\mu)^2$\\
$\sigma^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i-\mu)^2$\\

So...\\
$\hat{\sigma}^2 = \frac{1}{n}\sum_{i=1}^{n}(x_i-\hat{\mu})^2$

\end{answertext}

\end{enumerate}

% -----------------------------------------------------------
\pagebreak
\section{Conditional Independence [20 pts]}
A large group of people were surveyed on their recent health. Of these, 0.20 had a fever and 0.05 had pneumonia. Among the people who had pneumonia, 0.70 had cough as a symptom and 0.50 had fever as a symptom. Among the people who had a fever, 0.40 had cough as a symptom.

Let us create a probabilistic model where the presence/absence of each of these two symptoms, cough and fever, are conditionally independent given the presence/absence of pneumonia. Using this data for the empirical probabilities of our model, answer the following questions.
\begin{enumerate}
\item (5 pts) Find the probability that someone has both a cough and a fever.  \\
\begin{answertext}{6.5cm}{}

Let $C = cough, F = fever,$ and $P = pneumonia$\\
$p(C \cap F) = p(C \cap F \mid P)p(P) + p(C \cap F \mid \lnot P)p(\lnot P)$\\
$p(C \cap F \mid P) = p(C \mid P)p(F \mid P) = 0.7 \cdot 0.5 = 0.35$\\
$p(C \cap F \mid \lnot P) = p(C \mid \lnot P)p(F \mid \lnot P) = p(C \mid F) \cdot p(F \mid \lnot P)$\\
Find $p(F \mid \lnot P)$...\\
$p(F) = p(F \mid P) \cdot p(P) + p(F \mid \lnot P)\cdot p(\lnot P)$\\
$0.2 = 0.5 \cdot 0.05 + p(F \mid \lnot P) \cdot 0.95 \Longrightarrow p(F \mid \lnot P) \approx 0.1842$\\
$p(C \cap F \mid \lnot P) = .4 \cdot 0.1842 \approx 0.7368$\\
$p(C \cap F) = 0.35 \cdot 0.05 + 0.7368 \cdot 0.95 \approx 0.0875 = 8.75\%$
  
\end{answertext} 
\item (5 pts) Find the probability that someone has pneumonia given that they have a fever but no cough.  \\
\begin{answertext}{6cm}{}

Let $C = cough, F = fever,$ and $P = pneumonia$\\
$p(P \mid F, \lnot C) = \frac{p(F \cap \lnot C \mid P)p(P)}{p(F \cap \lnot C)}$\\
$p(F \cap \lnot C \mid P) = p(F \mid P)p(\lnot C \mid P) = 0.5 \cdot (1 - 0.7) = .15$\\
$p(F \cap \lnot C) = p(F \cap \lnot C \mid P)p(P) + p(F \cap \lnot C \mid \lnot P)$\\
$p(F \cap \lnot C \mid \lnot P) =  p(F \mid \lnot P)p(\lnot C \mid F) \approx 0.1842 \cdot (1 - 0.4) = 0.1105$\\
Note that $p(F \mid \lnot P)$ was found in the previous question.\\
$p(F \cap \lnot C) = 0.15(0.05) + 0.11052(0.95) \approx 0.1135$\\
$p(P \mid F \cap \lnot C) = \frac{p(F \cap \lnot C \mid P)p(P)}{p(F \cap \lnot C)} = \frac{0.15 \cdot 0.05}{0.1135} \approx 0.0661 = 6.61\%$

\end{answertext} 

\newpage

\item (10 pts) Given assumptions described above, how many parameters do we need to specify the joint distribution $p(fever, cough, pneumonia)$?  \\
\begin{answertext}{6cm}{}

Let $C = cough, F = fever,$ and $P = pneumonia$\\
The total number of combinations of these variables i $2^3 = 8$.\\
$p(F, C, P) = p(P)p(F \mid P)p(C \mid F, P)$\\
$p(C \mid F, P) = p(C \mid P)$ Due to conditional independence.\\
Counting the parameters...\\
$p(P) \rightarrow $ 1 parameter (it is a binary variable)\\
$p(F \mid P) \rightarrow $ 2 parameters ($P$ may be $\lnot P$ as well, giving two binary variables)\\
$p(C \mid P) \rightarrow $ 2 parameters for identical reason as above.\\
The sum of these results in $5$ total parameters.
        
\end{answertext} 
\end{enumerate}


% -----------------------------------------------------------
\pagebreak
\section{Bayesian Reasoning [20 pts]}
\begin{enumerate}

\item (5 pts) Define what conjugate priors are, and explain why they are useful. \\
\begin{answertext}{3.5cm}{}  

A conjugate prior is a prior distribution that belongs to the same probability distribution of a posterior distribution for a likelihood function. $p(\theta \mid x) \propto p(x \mid \theta)p(\theta)$\\

Using a conjugate prior is useful as the posterior distribution is more closely related to the prior distribution. This makes it easier for calculations, updating the posterior by adjusting the prior, and providing an easier interpretation for data.\\

\end{answertext} 

\item (10 pts) Show that the Gamma distribution is a conjugate prior of the exponential distribution. That is, show that if $x \sim \text{Exp}(\lambda)$ and $\lambda \sim \text{Gamma}(\alpha, \beta)$, then $p(\lambda | x) \sim \text{Gamma}(\alpha^*, \beta^*)$ for some $\alpha^*$, $\beta^*$. \\
\begin{answertext}{6cm}{}

Likelihood of $x$:\\
$p(x \mid \lambda) = \lambda e^{-\lambda x}$ for $x \geq 0$\\
Gamma prior:\\
$p(\lambda) = \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda}$ for $\lambda > 0$\\
Bayes' theorem:\\
$p(\lambda \mid x) \propto p(x \mid \lambda)p(\lambda) = (\lambda e^{-\lambda x}) \cdot (\frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^{\alpha-1}e^{-\beta\lambda})$\\
$ = \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^\alpha e^{-\lambda(x+\beta)}$\\

We can see that $\alpha^* = \alpha + 1$ and $\beta^* = \beta + x$. So...\\
$p(\lambda \mid x) ~ Gamma(\alpha + 1, \beta + x)$

\end{answertext} 

%    \item Let us assume that \(\lambda\) follows a Gamma distribution with parameters \(\alpha, \beta\) (this is the prior distribution):
%    \begin{equation*}
%        p\left(\lambda | \alpha, \beta\right) = \frac{\beta^\alpha}{\Gamma\left(\alpha\right)} \lambda^{\alpha - 1} e^{-\beta\lambda}
%    \end{equation*}
%    Compute the posterior distribution over \(\lambda\). (Hint: the denominator is a constant).

%\pagebreak

    \item (5 pts) Derive the maximum a posteriori (MAP) of \(\lambda\) under the \(Gamma\left(\alpha, \beta\right)\) prior. \\
\begin{answertext}{6cm}{}

Maximum likelihood estimate of $\hat{\lambda}$\\
$\hat{\lambda}_{MAP}(x) = \argmax_{\lambda} f(\lambda \mid x)$\\
From last question, we see $f(\lambda \mid x) \propto \frac{\beta^\alpha}{\Gamma(\alpha)}\lambda^\alpha e^{-\lambda(x+\beta)}$\\
$\log p(\lambda \mid x) = \alpha \log(\lambda) - \lambda(x + \beta) + C$ where $C$ is the constant term.\\
$\frac{d}{d\lambda}\log p(\lambda \mid x) = \frac{\alpha}{\lambda}-(x+\beta)$
$0 = \frac{\alpha}{\lambda}-(x+\beta) \Longrightarrow \lambda = \frac{\alpha}{x + \beta}$\\

$\hat{\lambda}_{MAP} = \frac{\alpha}{x+\beta}$

\end{answertext} 

\end{enumerate}

\end{document}
