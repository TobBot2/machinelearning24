{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQmp-ad0hZsO"
      },
      "source": [
        "# **Homework 2 Practicum**\n",
        "# Version 1.0 (September 20, 2024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slq1QivyhZsP"
      },
      "source": [
        "<font color='blue'> TODO:</font> Name (JHED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FV22tFvMhZsP"
      },
      "source": [
        "Instructions:\n",
        "This notebook has two parts:\n",
        "\n",
        "Part 1: Exploring Regularization with Polynomial Regression\n",
        "\n",
        "In this part, you will implement Polynomial Regression and explore the impact of regularization on the model.\n",
        "\n",
        "Part 2: Implement Logistic Regression using Newton-Raphson Method\n",
        "\n",
        "In this part, you will implement Logistic Regression and explore the impact of regularization on the model.\n",
        "\n",
        "Please answer all questions in this notebook (you will see <font color='blue'>TODO</font> annotations for where to include your answers). At the beginning of each part, we will bullet the expected deliverables for you to complete.\n",
        "\n",
        "Please <font color='blue'>make a copy of this notebook in your own drive</font> before you make any edits. You can do so through File -> Save a copy in Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dq6iPbgVhZsP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b-odGeZdhZsQ"
      },
      "source": [
        "\n",
        "\n",
        "# **PART I: Exploring Regularization with Polynomial Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "aotAcGOehZsR"
      },
      "source": [
        "Things to do in this part:\n",
        "1. Implement the MyPolynomialRegression class\n",
        "2. Answer the two questions\n",
        "\n",
        "In this section, you will implement a polynomial regression model from scratch and explore how regularization affects its performance. You will work with a synthetic dataset and learn to:\n",
        "\n",
        "- Implement a custom polynomial regression class: You will complete the MyPolynomialRegression class by filling in the  fit, predict and calculate_error methods.\n",
        "- Analyze the impact of polynomial degree: You will experiment with different polynomial degrees to see how model complexity affects the bias-variance trade-off and the risk of overfitting. Note that the code for plots  is written and you just need to run it.\n",
        "- Investigate the role of regularization: You will vary regularization coefficient for L2 regularization of the polynomial regression and observe its effect on model performance, particularly in reducing overfitting. Note that the code for plots  is written and you just need to run it.\n",
        "- Answer questions: Based on steps 2 and 3  analyze the plots, note the observations and provide explanations for the observed trends.\n",
        "\n",
        "By the end of this section, you will have a solid grasp of polynomial regression, overfitting, and the role of regularization in building effective machine learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_d53S1uUhZsR"
      },
      "outputs": [],
      "source": [
        "#Run this without modifying\n",
        "#Generating Synthetic Data for Polynomial Regressoion\n",
        "def true_fn(X):\n",
        "    return  0.7* X **4 + 0.25* X ** 3 + 0.75 * X ** 2 - 1.5 * X  + 2\n",
        "\n",
        "\n",
        "def generate_data(n,mean=0,sd=1):\n",
        "    X = np.sort(np.random.uniform(0,1,n))\n",
        "    y = true_fn(X) + (np.random.normal(loc=mean, scale=sd, size=n)*0.1)\n",
        "    return X,y\n",
        "\n",
        "X_train,y_train=generate_data(30)\n",
        "X_test,y_test=generate_data(30)\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "ax1.scatter(X_train,y_train)\n",
        "ax1.set_xlabel(\"X_train\")\n",
        "ax1.set_ylabel(\"y_train\")\n",
        "ax1.set_title(\"Training Data\")\n",
        "\n",
        "ax2.scatter(X_test,y_test)\n",
        "ax2.set_xlabel(\"X_test\")\n",
        "ax2.set_ylabel(\"y_test\")\n",
        "ax2.set_title(\"Test Data\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Zk8lKVTWhZsS"
      },
      "source": [
        "Below you  need to **implement** fit, predict and calculate_error methods of MyPolynomialRegression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZRCap25hZsS",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "class MyPolynomialRegression:\n",
        "    \"\"\"\n",
        "    A class to perform polynomial regression and regularization.\n",
        "    \"\"\"\n",
        "    def __init__(self, degree, reg_coeff):\n",
        "        \"\"\"\n",
        "        Initializes a MyPolynomialRegression object.\n",
        "\n",
        "        Parameters:\n",
        "        degree (int): The degree of the polynomial regression.\n",
        "        reg_coeff (float): The regularization coefficient.\n",
        "\n",
        "        Attributes:\n",
        "        self.degree (int): The degree of the polynomial regression.\n",
        "        self.reg_coeff (float): The regularization coefficient.\n",
        "        self.weights (list): The weights of the polynomial regression (initialized as None).\n",
        "        \"\"\"\n",
        "        self.degree = degree\n",
        "        self.reg_coeff = reg_coeff\n",
        "        self.weights = None\n",
        "\n",
        "    def _gen_polynomial_features(self,X):\n",
        "        \"\"\"\n",
        "        Generates the polynomial feature matrix from the input data X.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy array): The input data.\n",
        "\n",
        "        Returns:\n",
        "        poly_X (numpy array): The polynomial feature matrix.\n",
        "\n",
        "        Notes:\n",
        "        - The polynomial feature matrix is generated by raising the input data X to powers from 1 to self.degree.\n",
        "        - A column of ones is added to the feature matrix to account for the bias term.\n",
        "        \"\"\"\n",
        "        # TODO ...WRITE YOUR CODE HERE...\n",
        "        ...\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Fits for the given feature matrix and labels.\n",
        "\n",
        "        Parameters:\n",
        "        - X: array-like, shape (n_samples,), feature matrix.\n",
        "        - y: array-like, shape (n_samples,),  target values.\n",
        "        returns:\n",
        "        None\n",
        "        Note: should generate and assign values to self.weights\n",
        "        \"\"\"\n",
        "        # TODO ...WRITE YOUR CODE HERE...\n",
        "        ...\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self,X):\n",
        "        \"\"\"\n",
        "        Predicts target values for the given feature matrix.\n",
        "\n",
        "        Parameters:\n",
        "        - X: array-like, shape (n_samples,), feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        - y_pred: array-like, shape (n_samples,), predicted target values.\n",
        "        \"\"\"\n",
        "        # TODO ...WRITE YOUR CODE HERE...\n",
        "        ...\n",
        "\n",
        "    def calculate_error(self,y_pred,y_true):\n",
        "        \"\"\"\n",
        "        Calculates the mean squared error between predicted and true target values.\n",
        "\n",
        "        Parameters:\n",
        "        - y_pred: array-like, shape (n_samples,), predicted target values.\n",
        "        - y_true: array-like, shape (n_samples,), true target values.\n",
        "\n",
        "        Returns:\n",
        "        - mse: float, the mean squared error between y_pred and y_true.\n",
        "        \"\"\"\n",
        "        # TODO ...WRITE YOUR CODE HERE...\n",
        "        ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "JY0ybWMqhZsV"
      },
      "source": [
        "The provided code tests the polynomial regression implementation. You can add more tests to verify your implementation; these additional tests will not be graded, but are encouraged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6H0zK693hZsV"
      },
      "outputs": [],
      "source": [
        "\n",
        "test_model=MyPolynomialRegression(3,0)\n",
        "test_model.fit(X_train,y_train)\n",
        "test_preds=test_model.predict(X_test)\n",
        "test_preds_error=test_model.calculate_error(test_preds,y_test)\n",
        "print(f\"Test error with degree=3 and without regularizations is {test_preds_error:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "dX59Sl-2hZsW"
      },
      "source": [
        "Below code trains polynomial regression models with varying degrees and regularization,\n",
        "plots the results, and displays the training and testing errors.\n",
        "\n",
        "The plot_fit function iterates through a list of degrees, fitting both regularized and\n",
        "non-regularized models for each degree. It then generates plots to visualize\n",
        "the model fit, the true function, and the training and testing data.\n",
        "\n",
        "The plots are displayed in a grid, with each row representing a different degree\n",
        "and each column showing the results with and without regularization.\n",
        "\n",
        "The function also calculates and displays the training and testing errors for each model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "_vr95ktghZsW"
      },
      "outputs": [],
      "source": [
        "#Run this without modifying after implementing MyPolynomialRegression class above\n",
        "\n",
        "\n",
        "degrees=[i for i in range(1,13,2)]\n",
        "train_errors=[]\n",
        "test_errors=[]\n",
        "rtrain_errors=[]\n",
        "rtest_errors=[]\n",
        "def plot_fit():\n",
        "    fig, axs = plt.subplots(len(degrees),2,figsize=(16, 40))\n",
        "    for i,degree in enumerate(degrees):\n",
        "        #Train, predict and calculate error without regularization\n",
        "        model=MyPolynomialRegression(degree,0)\n",
        "        model.fit(X_train,y_train)\n",
        "        y_train_preds=model.predict(X_train)\n",
        "        y_test_preds=model.predict(X_test)\n",
        "        training_error=model.calculate_error(y_train_preds,y_train)\n",
        "        testing_error=model.calculate_error(y_test_preds,y_test)\n",
        "        train_errors.append(training_error)\n",
        "        test_errors.append(testing_error)\n",
        "\n",
        "        #Train, predict and calculate error with regularization\n",
        "        rmodel=MyPolynomialRegression(degree,0.2)\n",
        "        rmodel.fit(X_train,y_train)\n",
        "        ry_train_preds=rmodel.predict(X_train)\n",
        "        ry_test_preds=rmodel.predict(X_test)\n",
        "        rtraining_error=rmodel.calculate_error(ry_train_preds,y_train)\n",
        "        rtesting_error=rmodel.calculate_error(ry_test_preds,y_test)\n",
        "        rtrain_errors.append(rtraining_error)\n",
        "        rtest_errors.append(rtesting_error)\n",
        "\n",
        "\n",
        "        #plot them side by side\n",
        "        #plot non regularized\n",
        "        axs[i,0].scatter(X_train, y_train,color='blue',s=20 ,label='Sample')\n",
        "        axs[i,0].plot(X_test, true_fn(X_test),color='blue' ,label='True function')\n",
        "        axs[i,0].plot(X_test, y_test_preds,color='red',label=f'Model (Degree {degree})')\n",
        "        axs[i,0].set_xlabel('X')\n",
        "        axs[i,0].set_ylabel('y')\n",
        "        axs[i,0].set_title(f\"Without Regularization, Degree: {degree:2d}, Training error = {training_error:.3f} Testing error = {testing_error:.3f}\")\n",
        "        axs[i,0].legend()\n",
        "\n",
        "        #plot regularized\n",
        "        axs[i,1].scatter(X_train, y_train,color='blue',s=20 ,label='Sample')\n",
        "        axs[i,1].plot(X_test, true_fn(X_test),color='blue' ,label='True function')\n",
        "        axs[i,1].plot(X_test, ry_test_preds,color='red',label=f'Model (Degree {degree})')\n",
        "        axs[i,1].set_xlabel('X')\n",
        "        axs[i,1].set_ylabel('y')\n",
        "        axs[i,1].set_title(f\"With Regularization, Degree: {degree:2d}, Training error = {rtraining_error:.3f} Testing error = {rtesting_error:.3f}\")\n",
        "        axs[i,1].legend()\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "XGE8ZDS5hZsX"
      },
      "source": [
        "# **Question1: What do you observe from the plots? Discuss the reasons for the trends in your obeservations?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "X4keMKnmhZsX"
      },
      "source": [
        "<font color='blue'>\n",
        "    TODO: replace this cell with your answer\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "P6tg6y3MhZsX"
      },
      "source": [
        "Below we explore the impact of different regularization coefficients on polynomial regression models.\n",
        "\n",
        "The explore_reg_coeffs function iterates through a range of polynomial degrees and regularization coefficients.\n",
        "For each combination, it trains a polynomial regression model, calculates the training and testing errors, and generates visualizations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "KbHT5Ks0hZsY"
      },
      "outputs": [],
      "source": [
        "#Run without modifying but only after implementing both MyPolynomialRegression class and the above code\n",
        "def explore_reg_coeffs():\n",
        "    for degree in degrees:\n",
        "        reg_test_errors=[]\n",
        "        reg_train_errors=[]\n",
        "        for i,reg_coeff in enumerate(reg_coeffs):\n",
        "            #Vary degrees with regularization\n",
        "            rmodel=MyPolynomialRegression(degree,reg_coeff)\n",
        "            rmodel.fit(X_train,y_train)\n",
        "            ry_train_preds=rmodel.predict(X_train)\n",
        "            ry_test_preds=rmodel.predict(X_test)\n",
        "            rtraining_error=rmodel.calculate_error(ry_train_preds,y_train)\n",
        "            rtesting_error=rmodel.calculate_error(ry_test_preds,y_test)\n",
        "            reg_train_errors.append(rtraining_error)\n",
        "            reg_test_errors.append(rtesting_error)\n",
        "            print(f\"For reg coeff of {reg_coeff:.2f} training error is {rtraining_error:.3f} testing error is{rtesting_error:.3f}\")\n",
        "        plt.plot(reg_coeffs,reg_train_errors,label=f'Training Error')\n",
        "        plt.plot(reg_coeffs,reg_test_errors,label=f'Testing Error')\n",
        "        plt.xlabel('Regularization coefficient')\n",
        "        plt.ylabel('Error')\n",
        "        plt.title(f\"Impact of regularization coefficients for degree={degree} \")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "reg_coeffs=[0, 0.02,0.05, 0.1, 0.15, 0.2, 0.25, 0.3]\n",
        "degrees=[3,4,5,6,7]\n",
        "explore_reg_coeffs()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fGYIbaXyhZsY"
      },
      "source": [
        "# **Question 2: What do you observe from the plots? Discuss the reasons for the trends in your obeservations?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "B4pmrU1ghZsY"
      },
      "source": [
        "<font color='blue'>\n",
        "    TODO: replace this cell with your answer\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "Q2jk-mh5hZsZ"
      },
      "source": [
        "# **PART II: Implement Logistic Regression with Newton Raphson Method**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "SsEUtfERhZsZ"
      },
      "source": [
        "**Things to do in this part**:\n",
        "\n",
        "1. Implement MyLogisticRegression\n",
        "2. Answer Question 3\n",
        "\n",
        "In this section, you will implement a Logistic Regression model from scratch using the Newton-Raphson method for optimization. Logistic regression is a powerful technique for binary classification problems. The Newton-Raphson method is an iterative optimization algorithm used to find the best-fitting parameters for the model.\n",
        "\n",
        "You will work with the Breast Cancer Wisconsin (Diagnostic) dataset and learn to:\n",
        "\n",
        "1. Implement core components of the logistic regression class: You will complete the MyLogisticRegression class by filling in the fit method.\n",
        "2. Train and evaluate the model: You will train the logistic regression model on the training data and evaluate its performance.\n",
        "3. Visualize the convergence of model parameters: You will generate plots to track the convergence of the model's coefficients during the training process.Note that the code is provided.\n",
        "4. Answer questions: You will analyze the plots and provide explanations for the observed trends.\n",
        "\n",
        "By the end of this section, you will have a strong understanding of logistic regression, the Newton-Raphson optimization method, and the role of regularization in preventing overfitting in classification problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "UWlMAZuOhZsZ"
      },
      "outputs": [],
      "source": [
        "#Run without modifying, loading dataset for logistic regression\n",
        "data = load_breast_cancer()\n",
        "X_lr = data.data\n",
        "y_lr = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_lr_train, X_lr_test, y_lr_train, y_lr_test = train_test_split(X_lr, y_lr, test_size=0.2, random_state=42)\n",
        "print(f\"X_lr_train={X_lr_train.shape}\")\n",
        "print(f\"X_lr_test={X_lr_test.shape}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "m4LO7IpPhZsZ"
      },
      "source": [
        "Complete the implementation of fit method in MyLogisticRegression class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_WrK3mehZsa",
        "tags": []
      },
      "outputs": [],
      "source": [
        "\n",
        "class MyLogisticRegression:\n",
        "    def __init__(self, max_iters=10, tol=1e-8, reg_term=0):\n",
        "        \"\"\"\n",
        "        Initialize the logistic regression model.\n",
        "\n",
        "        Parameters:\n",
        "        max_iters (int): Maximum number of iterations for convergence.\n",
        "        tol (float): Convergence tolerance.\n",
        "        reg_term (float): Regularization term.\n",
        "        \"\"\"\n",
        "        self.max_iters = max_iters\n",
        "        self.tol = tol\n",
        "        self.reg_term = reg_term\n",
        "        self.weights = None\n",
        "        self.weights_history=[]\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        \"\"\"\n",
        "        Compute the sigmoid function.\n",
        "\n",
        "        Parameters:\n",
        "        x (numpy array): Input array.\n",
        "\n",
        "        Returns:\n",
        "        numpy array: Sigmoid output.\n",
        "        \"\"\"\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "    def _newton_step(self,old_weights,X,y):\n",
        "        \"\"\"\n",
        "        Perform a single Newton-Raphson step.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy array): Feature matrix.\n",
        "        y (numpy array): Target values.\n",
        "\n",
        "        Returns:\n",
        "        numpy array: Updated weights\n",
        "        \"\"\"\n",
        "        # TODO ...WRITE YOUR CODE HERE...\n",
        "        ...\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"\n",
        "        Train the logistic regression model.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy array): Feature matrix.\n",
        "        y (numpy array): Target values.\n",
        "        \"\"\"\n",
        "        # Add intercept term\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "        self.weights = np.zeros(X.shape[1])\n",
        "        self.weights_history.append(self.weights.copy())\n",
        "        converged = False\n",
        "        iteration = 0\n",
        "\n",
        "        while not converged and iteration < self.max_iters:\n",
        "            iteration += 1\n",
        "            old_weights=self.weights.copy()\n",
        "            #Update weights\n",
        "            self.weights=self._newton_step(old_weights,X,y)\n",
        "            #Storing weights for plotting\n",
        "            self.weights_history.append(self.weights.copy())\n",
        "            # Check for convergence\n",
        "            delta=self.weights-old_weights\n",
        "            if np.linalg.norm(delta) < self.tol:\n",
        "                converged = True\n",
        "                print(f'Converged after {iteration} iterations.')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Make predictions using the trained model.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy array): Feature matrix.\n",
        "\n",
        "        Returns:\n",
        "        numpy array: Predictions\n",
        "        \"\"\"\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "        probabilities = self.sigmoid(np.dot(X, self.weights))\n",
        "        return np.where(probabilities >= 0.5, 1, 0)\n",
        "\n",
        "    def test_model(self, X, y):\n",
        "        \"\"\"\n",
        "        Evaluate the model's accuracy.\n",
        "\n",
        "        Parameters:\n",
        "        X (numpy array): Feature matrix.\n",
        "        y (numpy array): Target values.\n",
        "\n",
        "        Returns:\n",
        "        float: Accuracy percentage.\n",
        "        \"\"\"\n",
        "\n",
        "        prob = self.predict(X)\n",
        "        count=0\n",
        "        for yp,yt in zip(prob,y):\n",
        "            if yp==yt:\n",
        "                count+=1\n",
        "        return count/len(y)\n",
        "\n",
        "    def plot_convergence(self):\n",
        "         \"\"\"\n",
        "          Plots the convergence of the model's coefficients (beta values) over iterations.\n",
        "\n",
        "          This function visualizes how the beta values change during the training process,\n",
        "          helping to understand the convergence behavior of the Newton-Raphson optimization.\n",
        "\n",
        "          It creates a line plot with iteration number on the x-axis and beta values on the y-axis.\n",
        "          Each beta coefficient is represented by a separate line, allowing for easy comparison\n",
        "          of their convergence patterns.\n",
        "\n",
        "          The plot includes labels for the axes, a title, and a legend to identify each beta coefficient.\n",
        "          \"\"\"\n",
        "\n",
        "         y_vals=np.array(self.weights_history)\n",
        "\n",
        "\n",
        "         for i in range(y_vals.shape[1]):\n",
        "            plt.plot(range(len(self.weights_history)), y_vals[:, i], label=f'Beta {i}')\n",
        "\n",
        "         plt.xlabel('Iteration')\n",
        "         plt.ylabel('Beta Values')\n",
        "         plt.title('Convergence of Coefficients')\n",
        "         plt.legend(\n",
        "            loc='upper left',\n",
        "            bbox_to_anchor=(1, 1),\n",
        "            ncol=5,\n",
        "            title='Legend',\n",
        "            fontsize='small',\n",
        "            title_fontsize='medium'\n",
        "         )\n",
        "         plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "DHAEofw-hZsa"
      },
      "source": [
        "We provide the code to test the logistic regression model on the tennis dataset. You can add more tests to verify your implementation; these additional tests will not be graded, but are encouraged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "JoAorjdchZsb"
      },
      "outputs": [],
      "source": [
        "lr_test_model=MyLogisticRegression(max_iters=10, tol=1e-8, reg_term=0)\n",
        "lr_test_model.fit(X_lr_train,y_lr_train)\n",
        "test_accuracy = lr_test_model.test_model(X_lr_test,y_lr_test)\n",
        "print(f\"Accuracy is {test_accuracy:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "mYYrtWe9hZsc"
      },
      "source": [
        "Below we provide code to train and evaluate logistic regression models with and without regularization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "qRGOjO7ghZsc"
      },
      "outputs": [],
      "source": [
        "# Run without modifying\n",
        "for reg_term in [0,1]:\n",
        "  lr_model = MyLogisticRegression(max_iters=10, tol=1e-8, reg_term=reg_term)\n",
        "\n",
        "  # Train the model\n",
        "  lr_model.fit(X_lr_train, y_lr_train)\n",
        "\n",
        "  # Evaluate the model's accuracy\n",
        "  train_accuracy=lr_model.test_model(X_lr_train,y_lr_train)\n",
        "  test_accuracy = lr_model.test_model(X_lr_test,y_lr_test)\n",
        "  print(f'After training  with regularizaton= {reg_term}, train accuracy is {train_accuracy:.3f} and  test accuracy is {test_accuracy:.3f}')\n",
        "  lr_model.plot_convergence()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "UxNyY4f9hZsc"
      },
      "source": [
        "# **Question 3: What do you observe from the plots? Discuss the reasons for the trends in your obeservations?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8ZW9EOo7hZsc"
      },
      "source": [
        "<font color='blue'>\n",
        "    TODO: replace this cell with your answer\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UyU_vfChZsd"
      },
      "source": [
        "## Feedback\n",
        "\n",
        "Please provide us with some feedback on how long each section or this homework overall took you. Any other feedback is also welcomed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kVbMEfNuhZsd"
      },
      "source": [
        "## Submit\n",
        "Great work! You're all done.\n",
        "\n",
        "Make sure to submit this Python notebook. See the homework writeup for directions."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "otter": {
      "OK_FORMAT": true,
      "assignment_name": "hw02",
      "tests": {
        "PolynomialRegression": {
          "name": "PolynomialRegression",
          "points": null,
          "suites": [
            {
              "cases": [
                {
                  "code": ">>> X1 = np.array([1, 2, 3])\n>>> y1 = np.array([1, 4, 9])\n>>> pr = MyPolynomialRegression(2, 0)\n>>> pr.fit(X1, y1)\n>>> expected_weights = np.array([0, 0, 1])\n>>> assert np.allclose(expected_weights, pr.weights, atol=0.1)\n",
                  "failure_message": "Test the fit method of the MyPolynomialRegression class.",
                  "hidden": false,
                  "locked": false,
                  "points": 1,
                  "success_message": "Test the fit method of the MyPolynomialRegression class."
                }
              ],
              "scored": true,
              "setup": "",
              "teardown": "",
              "type": "doctest"
            }
          ]
        }
      }
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}