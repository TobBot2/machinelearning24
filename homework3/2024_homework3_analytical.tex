\documentclass[11pt]{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage[fleqn]{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[many]{tcolorbox}
\usepackage{lipsum}
\usepackage{float}
\usepackage{trimclip}
\usepackage{listings}
\usepackage{environ}% http://ctan.org/pkg/environ
\usepackage{wasysym}
\usepackage{array}


\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm

\newcommand{\vwi}{{\bf w}_i}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}
\newcommand{\defeq}{\overset{\text{def}}{=}}
\renewcommand{\vec}[1]{\mathbf{#1}}



% \bgroup
\def\arraystretch{1.5}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{z}[1]{>{\centering\arraybackslash}m{#1}}

%Arguments are 1 - height, 2 - box title
\newtcolorbox{textanswerbox}[2]{%
 width=\textwidth,colback=white,colframe=blue!30!black,floatplacement=H,height=#1,title=#2,clip lower=true,before upper={\parindent0em}}

 \newtcolorbox{eqanswerbox}[1]{%
 width=#1,colback=white,colframe=black,floatplacement=H,height=3em,sharp corners=all,clip lower=true,before upper={\parindent0em}}

 %Arguments are 1 - height, 2 - box title
 \NewEnviron{answertext}[2]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
        \BODY
        \end{textanswerbox}
        }
        }
}

%Arguments are 1 - height, 2 - box title, 3 - column definition
 \NewEnviron{answertable}[3]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
                \vspace{-0.5cm}
                        \begin{table}[H]
                        \centering
                        \begin{tabular}{#3}
                                \BODY
                        \end{tabular}
                        \end{table}
        \end{textanswerbox}
        }
        }
}

 %Arguments are 1 - height, 2 - box title, 3 - title, 4- equation label, 5 - equation box width
 \NewEnviron{answerequation}[5]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
                \vspace{-0.5cm}
                        \begin{table}[H]
                        \centering
                \renewcommand{\arraystretch}{0.5}% Tighter

                        \begin{tabular}{#3}
                                #4 =	&
                        \clipbox{0pt 0pt 0pt 0pt}{

                        \begin{eqanswerbox}{#5}
                                $\BODY$
                        \end{eqanswerbox}
                        } \\
                        \end{tabular}
                        \end{table}

        \end{textanswerbox}
        }
        }
}

 %Arguments are 1 - height, 2 - box title
 \NewEnviron{answerderivation}[2]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
        \BODY
        \end{textanswerbox}
        }
        }
}

\newcommand{\Checked}{{\LARGE \XBox}}%
\newcommand{\Unchecked}{{\LARGE \Square}}%
\newcommand{\TextRequired}{{\textbf{Place Answer Here}}}%
\newcommand{\EquationRequired}{\textbf{Type Equation Here}}%


\newcommand{\answertextheight}{5cm}
\newcommand{\answertableheight}{4cm}
\newcommand{\answerequationheight}{2.5cm}
\newcommand{\answerderivationheight}{14cm}

\newcounter{QuestionCounter}
\newcounter{SubQuestionCounter}[QuestionCounter]
\setcounter{SubQuestionCounter}{1}

\newcommand{\subquestiontitle}{Question \theQuestionCounter.\theSubQuestionCounter~}
\newcommand{\newquestion}{\stepcounter{QuestionCounter}\setcounter{SubQuestionCounter}{1}\newpage}
\newcommand{\newsubquestion}{\stepcounter{SubQuestionCounter}}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\indices}{indices}
\DeclareMathOperator{\Bernoulli}{Bernoulli}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}

\lstset{language=[LaTeX]TeX,basicstyle=\ttfamily\bf}

\pagestyle{myheadings}
\markboth{Homework 3}{Fall 2024 CS 475/675 Machine Learning: Homework 3}

\title{CS 475 Machine Learning: Homework 3 Analytical \\
(50 points)\\
\Large{Assigned: Friday, October 04, 2024} \\
\Large{Due: Friday, October 18, 2024, 11:59 pm US/Eastern}}
\author{Trevor Black (tblack20)}
\date{}

\begin{document}
\maketitle
\thispagestyle{headings}

\section*{Instructions }
We have provided this \LaTeX{} document for turning in this homework. We give you one or more boxes to answer each question.  The question to answer for each box will be noted in the title of the box.  You can change the size of the box if you need more space.\\

{\bf Other than your name, do not type anything outside the boxes. Leave the rest of the document unchanged.}\\


\textbf{Do not change any formatting in this document, or we may be unable to
  grade your work. This includes, but is not limited to, the font sizes, and the spacing of text and tables.  Additionally, do
  not add text outside of the answer boxes. Entering your answers are the only
  changes allowed.}\\


\textbf{We strongly recommend you review your answers in the generated PDF to
  ensure they appear correct. We will grade what appears in the answer boxes in
  the submitted PDF, NOT the original latex file.}

% \section*{ Notation}
% {
% \centering
% \smallskip\begin{tabular}{r l}
% \(\vec{x_i}\) & One input data vector. \(\vec{x_i}\) is \(M\) dimensional.
% \(\vec{x_i} \in \mathbb{R}^{1 \times M}\).  \\ &
% We assume $\vec{x_i}$ is augmented with a  $1$ to include a bias term. \\ \\
% \(\vec{X}\) & 	A matrix of concatenated \(\vec{x_i}\)'s. There are \(N\) input vectors, so \(\vec{X} \in \mathbb{R}^{N \times M}\) \\ \\
% \(y_i\) & The true label for input vector \(\vec{x_i}\). In regression problems, \(y_i\) is continuous. \\ & In general ,\(y_i\) can be a vector, but for now we assume it's a scalar: \(y_i \in \mathbb{R}^1\). \\ \\

% \(\vec{y}\) & 	A vector of concatenated \(y_i\)'s. There are \(N\) input vectors, so \(\vec{y} \in \mathbb{R}^{N \times 1}\) \\ \\

% \(\vec{w}\) & A weight vector. We are trying to learn the elements of \(\vec{w}\). \\
% & \(\vec{w}\) is the same number of elements as \(\vec{x_i}\) because we will end up computing \\
% & the dot product \(\vec{x_i} \cdot \vec{w}\). \\
% & \(\vec{w} \in \mathbb{R}^{M \times 1}\). We assume the bias term is included in \(\vec{w}\). \\ \\

% \(h(\vec(x))\) & The true regression function that describes the data. \\ \\
 
% i.i.d. & Independently and identically distributed. \\ \\

% Bias-variance  & We can write \(E_D[(f(x, D) - h(x))^2]\) = \\
% decomposition  & \((E_D[f(x, D) - h(x))^2 + E_D[(f(x, D) - E_D[f(x, D)])^2]\) \\
%                             & where the first term is the bias squared, and the second term is the variance.\\ \\

%  Notes: & In general, a lowercase letter (not boldface), $a$, indicates a scalar. \\
%   & A boldface lowercase letter, $\vec{a}$, indicates a vector. \\  &  A boldface uppercase letter, $\vec{A}$, indicates a matrix. \\
% \end{tabular}
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Adaboost [25 pts]}
Assume a one-dimensional dataset with $x = \langle -1, 0, 1 \rangle$ and $y =
\langle -1, +1, -1 \rangle$.  Consider three weak classifiers:
\begin{align*}
  h_1(x) = \begin{cases}
    1  & \text{if } x > \frac{1}{2} \\
    -1 & \text{otherwise}
  \end{cases},
  \quad\quad
  h_2(x) = \begin{cases}
    1  & \text{if } x > - \frac{1}{2} \\
    -1 & \text{otherwise}
  \end{cases},
  \quad\quad
  h_3(x) = 1.
\end{align*}

\begin{enumerate}[(1)]

\item Show your work for the first $2$ iterations of \textsc{AdaBoost}. You must use the
  method from the lecture slides. In the
  table below, replace the ``?'' in the table below with the value of the
  quantity at iteration $t$.  $h_t \in \{1,2,3\}$, weak learner's weight in the
  ensemble, $\alpha_t$, error rate, $\varepsilon_t$.  Additionally, give the
  distribution over example at iteration $t$, $D_{t}(1), D_{t}(2), D_{t}(3)$.
  Use $\log$ with base $e$ in your calculations.

\begin{answertext}{8.2cm}{}

For the first iteration, start with predictions from $h_t(x)$.\\
\\ $h_1(x)$ predicts $\langle-1, -1, 1\rangle$, so the error is $2/3$. $h_2(x)$ predicts $\langle-1, 1, 1\rangle$, so the error is $1/3$. $h_3(x)$ predicts $\langle1, 1, 1\rangle$, so its error is $2/3$. $h_2(x)$ had the lowest error, so $h_1 = h_2(x)$ with an error rate of $\epsilon_1 = 1/3$ Computing the $\alpha$ value, we get $\alpha_1 = \frac{1}{2} \ln(\frac{1-\epsilon_1}{\epsilon_1}) = .347$\\

Moving on to the next iteration, we start by updating the values for $D_{2}(1), D_{2}(2), D_{2}(3)$ using the formula $D_{2}(i) = \frac{D_1(i)}{Z_1} \cdot \exp(-\alpha_1 y_i h_1(x_i))$. First, we find the normalizing factor $Z_1 = \Sigma D_1(i) \exp(-\alpha_1 y_i h_1(x_i)) = .472 + .236 + .472 = 1.18$. Now we can plug in everything to find $D_2(1) = .472 / 1.18 = .4, D_2(2) = .236 / 1.18 = .2, D_2(3) = .472 / 1.18 = .4$. \\

With these values found, we can proceded as before to find the new values for $h_2, \alpha_2,$ and $\epsilon_2$. The lowest error weak classifier is $h_1(x) = 0.2 + 0.4 = 0.6$ as $h_2(x)$ has already been chosen and $h_3(x)$ has a higher error rate. This lets us fill in our table with values $h_2 = 1$ and $\epsilon_2 = .6$. Computing $\alpha_2 = \frac{1}{2} \ln(\frac{1-\epsilon_2}{\epsilon_2}) = -.202$.

\end{answertext}

\begin{answertable}{3cm}{}{c|ccc|ccc}
$t$ & $h_t$ & $\alpha_t$ & $\varepsilon_t$ & $D_{t}(1)$ & $D_{t}(2)$ & $D_{t}(3)$ \\ \hline
1 & 2 & .347 & $ \frac{1}{3} $ & $ \frac{1}{3} $ & $ \frac{1}{3} $ & $ \frac{1}{3} $ \\
2 & 1 & -.202 & $ .6 $ & $ .4 $ & $ .2 $ & $ .4 $ \\
\end{answertable}
\item After running \textsc{AdaBoost} for 10 iterations, what is the error rate of the ensemble?

\begin{answertext}{3cm}{}

The ensemble error rate is defined as the fraction of data points that are classified incorrectly. It is bounded by $\prod_{t=1}^{T}2\sqrt{\epsilon_t(1-\epsilon_t)}$. We can assume that the classifier does better than random (especially after an iteration of AdaBoost). Basing off of the first value of $\epsilon_1 = .33$, we can approximate an upper bound the error rate to be $(2\sqrt{.33 \cdot .77})^{10} = .541$.

\end{answertext} 

\newsubquestion
\item In general, getting near-perfect training accuracy in machine learning
  leads to overfitting.  However, \textsc{AdaBoost} can get perfect training accuracy
  within overfitting.  Give a brief justification for why that is the case.
  
\begin{answertext}{5cm}{}

Typically, high training accuracy leads to overfitting because the model learns exactly the data it is trained on, so any minor deviations from that tend to spit out garbage. AdaBoost overcomes this by tending to maximize the margin. This ensures it doesn't conform too much to the training data and allows for small deviations in the input to not alter the output. It does this by altering the weights of the classifiers so that they only have an effect in smaller regions of the wider model. This feature of the algorithm allows it to maintain accuracy while avoiding overfitting.

\end{answertext} 
\pagebreak

\item The \textsc{AdaBoost} algorithm makes calls to a weighted classification
  solver which approximately solves the weighted 0/1-loss problem.\footnote{The
    approximation may come from minimizing an upper bound on 0/1 loss such as
    hinge loss.}  In other words, the classifier training algorithm is a
  function from a weighted dataset $\{ (w_i, x_i, y_i) \}_{i=1}^n$ to a
  classifier which approximately minimizes,
%
\begin{equation}\label{eq:weighted}
  \underset{h \in \mathcal{H}}{\mathrm{argmin}}\ \sum_{i=1}^n w_i \boldsymbol{1}(h(x_i) \ne y_i)
\end{equation}

  Suppose we have a new implementation of a classifier and want to use it in
  \textsc{AdaBoost}.  However, the classifier's training algorithm only accepts
  a dataset of $x$-$y$ pairs, $\{ (x_i, y_i) \}_{i=1}^n$ and thus it solves,
%
\begin{equation}\label{eq:unweighted}
  \underset{h \in \mathcal{H}}{\mathrm{argmin}}\ \frac{1}{n} \sum_{i=1}^n \boldsymbol{1}(h(x_i) \ne y_i)
\end{equation}

Give a give a principled method to \emph{approximate} the weighted
classification solution (\ref{eq:weighted}) given access to a training algorithm
that only accepts x-y pairs (\ref{eq:unweighted}).  Briefly sketch your idea and
why it is a reasonable approximation.

\begin{answertext}{10cm}{}

The key concept for the solution is to transform dataset into one that mimics the weighted version by resampling the dataset to include more of the heavily weighted datapoints. In other words, we must create an unweighted dataset where datapoints with higher weights are more likely to be sampled multiple times. This keeps the relative importance of the datapoints while removing the need for another parameter needing to be passed in.\\

In more detail, the original weighted dataset must be resampled to form a new dataset. To maintain that more heavily weighted datapoints keep their importance, the chance of any datapoint being sampled is dependent on its weight.\\

This approximation works because it keeps the relative importance of of each datapoint. It transfers that information from a separate 'weight' parameter to a frequency of occurences in the dataset.

\end{answertext}


\end{enumerate}

% -----------------------------------------------------------
\pagebreak

\section{Linear Classifier and Perceptron [25 pts]}
Given a dataset with two binary features, $X_1$ and $X_2$, the table below outlines all possible data points along with the results of applying the OR and AND functions to each example.
    $$
    \begin{tabular}{cc|cc}
    \hline
    $x_1$ & $x_2$ & OR & AND \\
    \hline
    0 & 0 & 0 & 0 \\
    0 & 1 & 1 & 0 \\
    1 & 0 & 1 & 0 \\
    1 & 1 & 1 & 1 \\
    \hline
    \end{tabular}
    $$
    Suppose there is a linear classifier with parameters $\boldsymbol{w}=(w_0, w_1, w_2)$ such that

    $$
        y = 
    \begin{cases} 
        1 & \text{if } w_1 x_1 + w_2 x_2 + w_0 \geq 0 \\
        0 & \text{otherwise}.
    \end{cases}
    $$

    \begin{enumerate}[(1)]
        \item Give two sets of valid values for $w_1, w_2$ and $w_0$ to represent the OR function. Show your work.
        \begin{answertext}{6cm}{}

To find a set of valid values for $w_1, w_2,$ and $w_0$ to represent the OR function, we start by setting the inequality $w_1 x_1 + w_2 x_2 + w_0 < 0 \iff x_1 = x_2 = 0$. This is true because based on the above table, we can see that a $0$ must be output if and only if both $x_1$ and $x_2$ are $0$. Plugging in our values, we see that $w_0 < 0$. Additionally, we see that if either value of $x_1$ or $x_2$ is $1$, the other condition must hold: $w_1 x_1 + w_2 x_2 + w_0 \geq 0$. From these constraints we can choose a negative value for $w_0$ that can be overcome by just $w_1$ or $w_2$ (no need for both). The following values fit that criteria: $w_0 = -1, w_1 = w_2 = 2$.\\

To find another set of valid values, we can easily modify $w_1$ and $w_2$ to be something larger, as any value for them larger than $-w_0$ will work. i.e. $w_0 = -1, w_1 = w_2 = 5$.

        \end{answertext}

        \item Give two sets of valid values for $w_1, w_2$ and $w_0$ to represent the AND function. Show your work.
        
        \begin{answertext}{6cm}{}

To find a set of valid values for $w_1, w_2,$ and $w_0$ to represent the AND function, we start by setting the inequality $w_1 x_1 + w_2 x_2 + w_0 \geq 0 \iff x_1 = x_2 = 1$. From this, we can set our values to find $w_1 + w_2 + w_0 \geq 0$. The setup also requires that $w_1 x_1 + w_2 x_2 + w_0 < 0$ if either value of $x$ is $0$. From this, we can see that $w_0$ must be $-2$, meaning $w_1$ and $w_2$ should each be $1$.\\

To find another set of valid values, we can easily multiply each parameter by the same constant value, i.e. $10$. So $w_0 = -20, w_1 = w_2 = 10$. In general, $w_1$ and $w_2$ must be between -1x and -0.5x the value of $w_0$ so that when both terms are present they can 'overcome' $w_0$. 

        \end{answertext}


        \item Can the Perceptron algorithm learn the above-defined OR and AND functions? Briefly explain.
        
        \begin{answertext}{3cm}{}

There is no perceptron algorithm that can learn the above-defined OR and AND functions because the constraints on them both are mutually exclusive. While they both require a negative value for $w_0$, the OR function requires $w_1$ and $w_2$ be larger than $-w_0$, while the AND function requires them to be less than $-w_0$.

        \end{answertext}
        
        \item We know that the XOR function is not linearly separable in two dimensions, but it can be made separable in higher dimensions with the addition of a suitable feature. Propose a new feature that can be added to the input space to make the XOR function linearly separable in three dimensions. Your answer should start with why XOR is not linearly separable in two dimensions, then define a new feature, give a new truth table, and finally demonstrate the feature effectively makes XOR linearly separable in three dimensions.

        \begin{answertext}{13cm}{}

XOR is not linearly separable in two dimensions as can be visually proven in a simple graph. In the below graph for XOR, we can see that the common outputs are diagonal to each other, so any hyperplane correctly grouping two of them together would inevitably incorrectly group another value with them.

    $$
    \begin{tabular}{c|cc}
    \hline
    XOR & $x_1 = 1$ & $x_2 = 1$ \\
    \hline
    $x_1 = 0$ & 1 & 0 \\
    $x_2 = 0$ & 0 & 1 \\
    \hline
    \end{tabular}
    $$

We can make the new feature $z = x_1 \cdot x_2$. This brings the problem space into the third dimension, allowing it to be linearly separable. The new truth table below, while cannot be represented in 3D, provides some insight on how this extra dimension allows the XOR function to be linearly separable.

    $$
    \begin{tabular}{ccc|c}
    \hline
    $x_1$ & $x_2$ & z & XOR \\
    \hline
    0 & 0 & 0 & 0 \\
    0 & 1 & 0 & 1 \\
    1 & 0 & 0 & 1 \\
    1 & 1 & 1 & 0 \\
    \hline
    \end{tabular}
    $$

This is linearly separable because the points $(0,0,0)$ and $(1,1,1)$ (both of which result in $0$) lie on the plane $z = x_1 \cdot x_2$. Additionally, points $(0,1,0)$ and $(1,0,0)$ lie on a different part of the 3D space. Because of this, we can define a plane to separate the classes, like $x_1 + x_2 - z \geq 0$ to classify a $1$ for the XOR function, and $0$ otherwise.

        \end{answertext}

        
    \end{enumerate}
% -----------------------------------------------------------
\pagebreak

\end{document}
