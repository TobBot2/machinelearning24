\documentclass[11pt]{article}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{url}
\usepackage{wrapfig}
\usepackage{color}
\usepackage{marvosym}
\usepackage{enumerate}
\usepackage{subfigure}
\usepackage{tikz}
\usepackage[fleqn]{amsmath}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage[many]{tcolorbox}
\usepackage{lipsum}
\usepackage{float}
\usepackage{trimclip}
\usepackage{listings}
\usepackage{environ}% http://ctan.org/pkg/environ
\usepackage{wasysym}
\usepackage{array}


\oddsidemargin 0mm
\evensidemargin 5mm
\topmargin -20mm
\textheight 240mm
\textwidth 160mm

\newcommand{\vwi}{{\bf w}_i}
\newcommand{\vw}{{\bf w}}
\newcommand{\vx}{{\bf x}}
\newcommand{\vy}{{\bf y}}
\newcommand{\vxi}{{\bf x}_i}
\newcommand{\yi}{y_i}
\newcommand{\vxj}{{\bf x}_j}
\newcommand{\vxn}{{\bf x}_n}
\newcommand{\yj}{y_j}
\newcommand{\ai}{\alpha_i}
\newcommand{\aj}{\alpha_j}
\newcommand{\X}{{\bf X}}
\newcommand{\Y}{{\bf Y}}
\newcommand{\vz}{{\bf z}}
\newcommand{\msigma}{{\bf \Sigma}}
\newcommand{\vmu}{{\bf \mu}}
\newcommand{\vmuk}{{\bf \mu}_k}
\newcommand{\msigmak}{{\bf \Sigma}_k}
\newcommand{\vmuj}{{\bf \mu}_j}
\newcommand{\msigmaj}{{\bf \Sigma}_j}
\newcommand{\pij}{\pi_j}
\newcommand{\pik}{\pi_k}
\newcommand{\D}{\mathcal{D}}
\newcommand{\el}{\mathcal{L}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\vxij}{{\bf x}_{ij}}
\newcommand{\vt}{{\bf t}}
\newcommand{\yh}{\hat{y}}
\newcommand{\code}[1]{{\footnotesize \tt #1}}
\newcommand{\alphai}{\alpha_i}
\newcommand{\defeq}{\overset{\text{def}}{=}}
\renewcommand{\vec}[1]{\mathbf{#1}}



\bgroup
\def\arraystretch{1.5}
\newcolumntype{x}[1]{>{\centering\arraybackslash\hspace{0pt}}p{#1}}
\newcolumntype{z}[1]{>{\centering\arraybackslash}m{#1}}

%Arguments are 1 - height, 2 - box title
\newtcolorbox{textanswerbox}[2]{%
 width=\textwidth,colback=white,colframe=blue!30!black,floatplacement=H,height=#1,title=#2,clip lower=true,before upper={\parindent0em}}

 \newtcolorbox{eqanswerbox}[1]{%
 width=#1,colback=white,colframe=black,floatplacement=H,height=3em,sharp corners=all,clip lower=true,before upper={\parindent0em}}

 %Arguments are 1 - height, 2 - box title
 \NewEnviron{answertext}[2]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
        \BODY
        \end{textanswerbox}
        }
        }
}

%Arguments are 1 - height, 2 - box title, 3 - column definition
 \NewEnviron{answertable}[3]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
                \vspace{-0.5cm}
                        \begin{table}[H]
                        \centering
                        \begin{tabular}{#3}
                                \BODY
                        \end{tabular}
                        \end{table}
        \end{textanswerbox}
        }
        }
}

 %Arguments are 1 - height, 2 - box title, 3 - title, 4- equation label, 5 - equation box width
 \NewEnviron{answerequation}[5]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
                \vspace{-0.5cm}
                        \begin{table}[H]
                        \centering
                \renewcommand{\arraystretch}{0.5}% Tighter

                        \begin{tabular}{#3}
                                #4 =	&
                        \clipbox{0pt 0pt 0pt 0pt}{

                        \begin{eqanswerbox}{#5}
                                $\BODY$
                        \end{eqanswerbox}
                        } \\
                        \end{tabular}
                        \end{table}

        \end{textanswerbox}
        }
        }
}

 %Arguments are 1 - height, 2 - box title
 \NewEnviron{answerderivation}[2]{
        \noindent
        \marginbox*{0pt 10pt}{
        \clipbox{0pt 0pt 0pt 0pt}{
        \begin{textanswerbox}{#1}{#2}
        \BODY
        \end{textanswerbox}
        }
        }
}

\newcommand{\Checked}{{\LARGE \XBox}}%
\newcommand{\Unchecked}{{\LARGE \Square}}%
\newcommand{\TextRequired}{{\textbf{Place Answer Here}}}%
\newcommand{\EquationRequired}{\textbf{Type Equation Here}}%


\newcommand{\answertextheight}{5cm}
\newcommand{\answertableheight}{4cm}
\newcommand{\answerequationheight}{2.5cm}
\newcommand{\answerderivationheight}{14cm}

\newcounter{QuestionCounter}
\newcounter{SubQuestionCounter}[QuestionCounter]
\setcounter{SubQuestionCounter}{1}

\newcommand{\subquestiontitle}{Question \theQuestionCounter.\theSubQuestionCounter~}
\newcommand{\newquestion}{\stepcounter{QuestionCounter}\setcounter{SubQuestionCounter}{1}\newpage}
\newcommand{\newsubquestion}{\stepcounter{SubQuestionCounter}}

\DeclareMathOperator{\rank}{rank}
\DeclareMathOperator{\indices}{indices}
\DeclareMathOperator{\Bernoulli}{Bernoulli}
\DeclareMathOperator{\Bin}{Bin}
\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}

\lstset{language=[LaTeX]TeX,basicstyle=\ttfamily\bf}

\pagestyle{myheadings}
\markboth{Homework 3}{Fall 2024 CS 475/675 Machine Learning: Homework 3}

\title{CS 475 Machine Learning: Homework 3 Analytical \\
(50 points)\\
\Large{Assigned: Friday, October 04, 2024} \\
\Large{Due: Friday, October 18, 2024, 11:59 pm US/Eastern}}
\author{NAME (JHED)}
\date{}

\begin{document}
\maketitle
\thispagestyle{headings}

\section*{Instructions }
We have provided this \LaTeX{} document for turning in this homework. We give you one or more boxes to answer each question.  The question to answer for each box will be noted in the title of the box.  You can change the size of the box if you need more space.\\

{\bf Other than your name, do not type anything outside the boxes. Leave the rest of the document unchanged.}\\


\textbf{Do not change any formatting in this document, or we may be unable to
  grade your work. This includes, but is not limited to, the font sizes, and the spacing of text and tables.  Additionally, do
  not add text outside of the answer boxes. Entering your answers are the only
  changes allowed.}\\


\textbf{We strongly recommend you review your answers in the generated PDF to
  ensure they appear correct. We will grade what appears in the answer boxes in
  the submitted PDF, NOT the original latex file.}

% \section*{ Notation}
% {
% \centering
% \smallskip\begin{tabular}{r l}
% \(\vec{x_i}\) & One input data vector. \(\vec{x_i}\) is \(M\) dimensional.
% \(\vec{x_i} \in \mathbb{R}^{1 \times M}\).  \\ &
% We assume $\vec{x_i}$ is augmented with a  $1$ to include a bias term. \\ \\
% \(\vec{X}\) & 	A matrix of concatenated \(\vec{x_i}\)'s. There are \(N\) input vectors, so \(\vec{X} \in \mathbb{R}^{N \times M}\) \\ \\
% \(y_i\) & The true label for input vector \(\vec{x_i}\). In regression problems, \(y_i\) is continuous. \\ & In general ,\(y_i\) can be a vector, but for now we assume it's a scalar: \(y_i \in \mathbb{R}^1\). \\ \\

% \(\vec{y}\) & 	A vector of concatenated \(y_i\)'s. There are \(N\) input vectors, so \(\vec{y} \in \mathbb{R}^{N \times 1}\) \\ \\

% \(\vec{w}\) & A weight vector. We are trying to learn the elements of \(\vec{w}\). \\
% & \(\vec{w}\) is the same number of elements as \(\vec{x_i}\) because we will end up computing \\
% & the dot product \(\vec{x_i} \cdot \vec{w}\). \\
% & \(\vec{w} \in \mathbb{R}^{M \times 1}\). We assume the bias term is included in \(\vec{w}\). \\ \\

% \(h(\vec(x))\) & The true regression function that describes the data. \\ \\
 
% i.i.d. & Independently and identically distributed. \\ \\

% Bias-variance  & We can write \(E_D[(f(x, D) - h(x))^2]\) = \\
% decomposition  & \((E_D[f(x, D) - h(x))^2 + E_D[(f(x, D) - E_D[f(x, D)])^2]\) \\
%                             & where the first term is the bias squared, and the second term is the variance.\\ \\

%  Notes: & In general, a lowercase letter (not boldface), $a$, indicates a scalar. \\
%   & A boldface lowercase letter, $\vec{a}$, indicates a vector. \\  &  A boldface uppercase letter, $\vec{A}$, indicates a matrix. \\
% \end{tabular}
% }
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagebreak
\section{Adaboost [25 pts]}
Assume a one-dimensional dataset with $x = \langle -1, 0, 1 \rangle$ and $y =
\langle -1, +1, -1 \rangle$.  Consider three weak classifiers:
\begin{align*}
  h_1(x) = \begin{cases}
    1  & \text{if } x > \frac{1}{2} \\
    -1 & \text{otherwise}
  \end{cases},
  \quad\quad
  h_2(x) = \begin{cases}
    1  & \text{if } x > - \frac{1}{2} \\
    -1 & \text{otherwise}
  \end{cases},
  \quad\quad
  h_3(x) = 1.
\end{align*}

\begin{enumerate}[(1)]

\item Show your work for the first $2$ iterations of \textsc{AdaBoost}. You must use the
  method from the lecture slides. In the
  table below, replace the ``?'' in the table below with the value of the
  quantity at iteration $t$.  $h_t \in \{1,2,3\}$, weak learner's weight in the
  ensemble, $\alpha_t$, error rate, $\varepsilon_t$.  Additionally, give the
  distribution over example at iteration $t$, $D_{t}(1), D_{t}(2), D_{t}(3)$.
  Use $\log$ with base $e$ in your calculations.

\begin{answertable}{3cm}{}{c|ccc|ccc}
$t$ & $h_t$ & $\alpha_t$ & $\varepsilon_t$ & $D_{t}(1)$ & $D_{t}(2)$ & $D_{t}(3)$ \\ \hline
1 & ? & ? & $ ? $ & $ \frac{1}{3} $ & $ \frac{1}{3} $ & $ \frac{1}{3} $ \\
2 & ? & ? & $ ? $ & $ ? $ & $ ? $ & $ ? $ \\
\end{answertable}
\item After running \textsc{AdaBoost} for 10 iterations, what is the error rate of the ensemble?

\begin{answertext}{3cm}{}

\end{answertext} 

\newsubquestion
\item In general, getting near-perfect training accuracy in machine learning
  leads to overfitting.  However, \textsc{AdaBoost} can get perfect training accuracy
  within overfitting.  Give a brief justification for why that is the case.
  
\begin{answertext}{5cm}{}

\end{answertext} 
\pagebreak

\item The \textsc{AdaBoost} algorithm makes calls to a weighted classification
  solver which approximately solves the weighted 0/1-loss problem.\footnote{The
    approximation may come from minimizing an upper bound on 0/1 loss such as
    hinge loss.}  In other words, the classifier training algorithm is a
  function from a weighted dataset $\{ (w_i, x_i, y_i) \}_{i=1}^n$ to a
  classifier which approximately minimizes,
%
\begin{equation}\label{eq:weighted}
  \underset{h \in \mathcal{H}}{\mathrm{argmin}}\ \sum_{i=1}^n w_i \boldsymbol{1}(h(x_i) \ne y_i)
\end{equation}

  Suppose we have a new implementation of a classifier and want to use it in
  \textsc{AdaBoost}.  However, the classifier's training algorithm only accepts
  a dataset of $x$-$y$ pairs, $\{ (x_i, y_i) \}_{i=1}^n$ and thus it solves,
%
\begin{equation}\label{eq:unweighted}
  \underset{h \in \mathcal{H}}{\mathrm{argmin}}\ \frac{1}{n} \sum_{i=1}^n \boldsymbol{1}(h(x_i) \ne y_i)
\end{equation}

Give a give a principled method to \emph{approximate} the weighted
classification solution (\ref{eq:weighted}) given access to a training algorithm
that only accepts x-y pairs (\ref{eq:unweighted}).  Briefly sketch your idea and
why it is a reasonable approximation.

\begin{answertext}{10cm}{}

\end{answertext}


\end{enumerate}

% -----------------------------------------------------------
\pagebreak

\section{Linear Classifier and Perceptron [25 pts]}
Given a dataset with two binary features, $X_1$ and $X_2$, the table below outlines all possible data points along with the results of applying the OR and AND functions to each example.
    $$
    \begin{tabular}{cc|cc}
    \hline
    $x_1$ & $x_2$ & OR & AND \\
    \hline
    0 & 0 & 0 & 0 \\
    0 & 1 & 1 & 0 \\
    1 & 0 & 1 & 0 \\
    1 & 1 & 1 & 1 \\
    \hline
    \end{tabular}
    $$
    Suppose there is a linear classifier with parameters $\boldsymbol{w}=(w_0, w_1, w_2)$ such that

    $$
        y = 
    \begin{cases} 
        1 & \text{if } w_1 x_1 + w_2 x_2 + w_0 \geq 0 \\
        0 & \text{otherwise}.
    \end{cases}
    $$

    \begin{enumerate}[(1)]
        \item Give two sets of valid values for $w_1, w_2$ and $w_0$ to represent the OR function. Show your work.
        \begin{answertext}{6cm}{}

        \end{answertext}

        \item Give two sets of valid values for $w_1, w_2$ and $w_0$ to represent the AND function. Show your work.
        
        \begin{answertext}{6cm}{}

        \end{answertext}


        \item Can the Perceptron algorithm learn the above-defined OR and AND functions? Briefly explain.
        
        \begin{answertext}{2cm}{}

        \end{answertext}
        
        \item We know that the XOR function is not linearly separable in two dimensions, but it can be made separable in higher dimensions with the addition of a suitable feature. Propose a new feature that can be added to the input space to make the XOR function linearly separable in three dimensions. Your answer should start with why XOR is not linearly separable in two dimensions, then define a new feature, give a new truth table, and finally demonstrate the feature effectively makes XOR linearly separable in three dimensions.

        \begin{answertext}{13cm}{}

        \end{answertext}

        
    \end{enumerate}
% -----------------------------------------------------------
\pagebreak

\end{document}
